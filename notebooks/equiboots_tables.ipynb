{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add path to import EquiBoots\n",
    "script_path = os.path.abspath(\"../py_scripts\")\n",
    "print(\"Appending path:\", script_path)\n",
    "sys.path.append(script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equiboots as eqb\n",
    "\n",
    "from equiboots.tables import metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_biased_synthetic_data(n_samples=1000, bias_strength='moderate', random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with intentional bias to create statistically significant differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of samples to generate\n",
    "    - bias_strength: 'mild', 'moderate', or 'strong' - controls the level of bias\n",
    "    - random_seed: For reproducibility\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Define bias parameters based on strength\n",
    "    bias_params = {\n",
    "        'mild': {'race_bias': 0.15, 'sex_bias': 0.08, 'noise_level': 0.3},\n",
    "        'moderate': {'race_bias': 0.25, 'sex_bias': 0.15, 'noise_level': 0.2},\n",
    "        'strong': {'race_bias': 0.4, 'sex_bias': 0.25, 'noise_level': 0.1}\n",
    "    }\n",
    "    \n",
    "    params = bias_params[bias_strength]\n",
    "    \n",
    "    # Generate demographic variables\n",
    "    race = np.random.choice([\"white\", \"black\", \"asian\", \"hispanic\"], n_samples, \n",
    "                           p=[0.4, 0.3, 0.15, 0.15]).reshape(-1, 1)\n",
    "    sex = np.random.choice([\"M\", \"F\"], n_samples, p=[0.5, 0.5]).reshape(-1, 1)\n",
    "    \n",
    "    # Create bias mappings\n",
    "    race_bias_map = {\n",
    "        \"white\": 0.0,      # baseline\n",
    "        \"black\": -params['race_bias'],    # disadvantaged\n",
    "        \"asian\": params['race_bias'] * 0.5,  # slight advantage\n",
    "        \"hispanic\": -params['race_bias'] * 0.7  # disadvantaged\n",
    "    }\n",
    "    \n",
    "    sex_bias_map = {\n",
    "        \"M\": params['sex_bias'] * 0.5,   # slight advantage\n",
    "        \"F\": -params['sex_bias'] * 0.5   # slight disadvantage\n",
    "    }\n",
    "    \n",
    "    # Generate base probabilities with bias\n",
    "    base_prob = 0.5  # neutral starting point\n",
    "    \n",
    "    # Apply demographic biases\n",
    "    race_adjustments = np.array([race_bias_map[r[0]] for r in race])\n",
    "    sex_adjustments = np.array([sex_bias_map[s[0]] for s in sex])\n",
    "    \n",
    "    # Combine biases with some noise\n",
    "    noise = np.random.normal(0, params['noise_level'], n_samples)\n",
    "    \n",
    "    # Calculate biased probabilities\n",
    "    y_prob = base_prob + race_adjustments + sex_adjustments + noise\n",
    "    \n",
    "    # Clip to valid probability range\n",
    "    y_prob = np.clip(y_prob, 0.01, 0.99)\n",
    "    \n",
    "    # Generate predictions and true labels based on biased probabilities\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Make true labels correlated with the biased probabilities to simulate real bias\n",
    "    # Add some randomness to make it realistic\n",
    "    true_label_prob = y_prob * 0.8 + np.random.uniform(0, 0.4, n_samples)\n",
    "    true_label_prob = np.clip(true_label_prob, 0.01, 0.99)\n",
    "    y_true = np.random.binomial(1, true_label_prob)\n",
    "    \n",
    "    return y_true, y_prob, y_pred, race, sex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob, y_pred, race, sex = generate_biased_synthetic_data(\n",
    "    n_samples=1000, \n",
    "    bias_strength='moderate',  # Try 'mild', 'moderate', or 'strong'\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Create fairness DataFrame\n",
    "fairness_df = pd.DataFrame(\n",
    "    data=np.concatenate((race, sex), axis=1), \n",
    "    columns=[\"race\", \"sex\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize and process groups\n",
    "eq = eqb.EquiBoots(\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    y_pred=y_pred,\n",
    "    fairness_df=fairness_df,\n",
    "    fairness_vars=[\"race\", \"sex\"],\n",
    ")\n",
    "eq.grouper(groupings_vars=[\"race\", \"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_race_data = eq.slicer(\"race\")\n",
    "race_metrics = eq.get_metrics(sliced_race_data)\n",
    "\n",
    "sliced_sex_data = eq.slicer(\"sex\")\n",
    "sex_metrics = eq.get_metrics(sliced_sex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    \"test_type\": \"chi_square\",\n",
    "    \"alpha\": 0.05,\n",
    "    \"adjust_method\": \"bonferroni\",\n",
    "    \"confidence_level\": 0.95,\n",
    "    \"classification_task\": \"binary_classification\",\n",
    "}\n",
    "stat_test_results_race = eq.analyze_statistical_significance(\n",
    "    race_metrics, \"race\", test_config\n",
    ")\n",
    "\n",
    "stat_test_results_sex = eq.analyze_statistical_significance(\n",
    "    sex_metrics, \"sex\", test_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_test_results_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_test_results_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stat_results = {\"sex\": stat_test_results_sex, \"race\": stat_test_results_race}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with custom y_lim and adjusted thresholds\n",
    "eqb.eq_group_metrics_point_plot(\n",
    "    group_metrics=[race_metrics, sex_metrics],\n",
    "    metric_cols=[\n",
    "        \"Accuracy\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "    ],\n",
    "    category_names=[\"race\", \"sex\"],\n",
    "    figsize=(6, 8),\n",
    "    include_legend=True,\n",
    "    plot_thresholds=(0.9, 1.1),\n",
    "    raw_metrics=True,\n",
    "    show_grid=True,\n",
    "    y_lim=(0, 1),\n",
    "    statistical_tests=overall_stat_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metrics_table_point = metrics_table(race_metrics, statistical_tests=stat_test_results_race, reference_group=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metrics_table_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list = np.linspace(0, 100, num=10, dtype=int).tolist()\n",
    "eq2 = eqb.EquiBoots(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    fairness_df,\n",
    "    [\"race\", \"sex\"],\n",
    "    y_prob,\n",
    "    seeds=int_list,\n",
    "    reference_groups=[\"white\", \"M\"],\n",
    "    task=\"binary_classification\",\n",
    "    bootstrap_flag=True,\n",
    "    num_bootstraps=1000,\n",
    "    boot_sample_size=1000,\n",
    "    balanced=True,  # False is stratified, True is balanced\n",
    "    # stratify_by_outcome=True,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "eq2.set_fix_seeds(int_list)\n",
    "print(\"seeds\", eq2.seeds)\n",
    "\n",
    "eq2.grouper(groupings_vars=[\"race\", \"sex\"])\n",
    "\n",
    "boots_race_data = eq2.slicer(\"race\")\n",
    "race_metrics = eq2.get_metrics(boots_race_data)\n",
    "dispa = eq2.calculate_disparities(race_metrics, \"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = eq2.calculate_differences(race_metrics, \"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_boot = ['Accuracy_diff', \"Precision_diff\", \"Recall_diff\"]\n",
    "\n",
    "\n",
    "test_config = {\n",
    "    \"test_type\": \"bootstrap_test\",\n",
    "    \"alpha\": 0.05,\n",
    "    \"adjust_method\": \"bonferroni\",\n",
    "    \"confidence_level\": 0.95,\n",
    "    \"classification_task\": \"binary_classification\",\n",
    "    \"tail_type\": \"two_tailed\",\n",
    "    \"metrics\": metrics_boot,\n",
    "}\n",
    "\n",
    "stat_test_results = eq.analyze_statistical_significance(\n",
    "    race_metrics, \"race\", test_config, diffs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqb.eq_group_metrics_plot(\n",
    "    group_metrics=diffs,\n",
    "    metric_cols=[\n",
    "        \"Accuracy_diff\",\n",
    "        \"Recall_diff\",\n",
    "        \"ROC_AUC_diff\"\n",
    "    ],\n",
    "    name=\"race\",\n",
    "    categories=\"all\",\n",
    "    figsize=(12, 4),\n",
    "    plot_type=\"violinplot\",\n",
    "    color_by_group=True,\n",
    "    show_grid=True,\n",
    "    strict_layout=True,\n",
    "    save_path=\"./images\",\n",
    "    show_pass_fail=False,\n",
    "    statistical_tests=stat_test_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_metrics_table_diff = metrics_table(race_metrics, statistical_tests=stat_test_results, differences=diffs, reference_group=\"white\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equiboots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
