

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Point Estimate Evaluation &mdash; EquiBoots 0.0.1a documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.js" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8c8a9e14"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=a0ccc04e"></script>
      <script src="_static/custom.js"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bootstrapped Metrics Evaluation" href="bootstrapped_metrics.html" />
    <link rel="prev" title="Welcome to the EquiBoots Documentation!" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            EquiBoots
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Welcome to the EquiBoots Documentation!</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Point Estimate Metrics</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Point Estimate Evaluation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#supported-metrics">Supported Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initial-set-up">Initial Set-up</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-import-and-initialize-equiboots"><strong>Step 1: Import and Initialize EquiBoots</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-slice-groups-and-compute-point-estimates"><strong>Step 2: Slice Groups and Compute Point Estimates</strong></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#metrics-dataframe">Metrics DataFrame</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#metrics_dataframe"><code class="docutils literal notranslate"><span class="pre">metrics_dataframe()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#statistical-tests">Statistical Tests</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#analyze_statistical_significance"><code class="docutils literal notranslate"><span class="pre">analyze_statistical_significance()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#statistical-significance-plots">Statistical Significance Plots</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#test-setup">Test Setup</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#statistical-metrics-table">Statistical Metrics table</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#metrics_table"><code class="docutils literal notranslate"><span class="pre">metrics_table()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Bootstrapped Metrics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bootstrapped_metrics.html">Bootstrapped Metrics Evaluation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">From Model to Prediction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_to_pred.html">Preparing Model Outputs for Fairness Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About EquiBoots</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributors.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="citations.html">Citing EquiBoots</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">EquiBoots</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Point Estimate Evaluation</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="point-estimate-evaluation">
<span id="point-estimates"></span><h1>Point Estimate Evaluation<a class="headerlink" href="#point-estimate-evaluation" title="Link to this heading"></a></h1>
<p>After training a model and preparing predictions, EquiBoots can be used to
evaluate how your model performs across different demographic groups. The most
basic step in this process is calculating point estimates. These are performance
metrics for each group without resampling or bootstrapping.</p>
<p>EquiBoots supports the computation of group-specific and overall point estimates
for performance metrics across classification and regression tasks. These estimates
form the basis for fairness auditing by revealing how models perform across
different subpopulations or sensitive attributes.</p>
<p>This section demonstrates how to compute group-wise performance metrics using
model outputs and fairness variables from the Adult Income dataset <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. For
bootstrapped confidence intervals, refer to the <a class="reference internal" href="bootstrapped_metrics.html#bootstrapped-metrics"><span class="std std-ref">bootstrapped metrics
evaluation section</span></a>.</p>
<section id="supported-metrics">
<h2>Supported Metrics<a class="headerlink" href="#supported-metrics" title="Link to this heading"></a></h2>
<p>For classification tasks, the following metrics are supported:</p>
<ul class="simple">
<li><p>Accuracy, Precision, Recall, F1-score</p></li>
<li><p>AUROC, AUPRC (for probabilistic models)</p></li>
<li><p>Calibration Area Under The Curve</p></li>
<li><p>Log Loss, Brier Score</p></li>
</ul>
<p>For regression tasks:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R^2, MAE, MSE, RMSE\)</span></p></li>
<li><p>Group-based residual plots</p></li>
</ul>
</section>
<section id="initial-set-up">
<h2>Initial Set-up<a class="headerlink" href="#initial-set-up" title="Link to this heading"></a></h2>
<section id="step-1-import-and-initialize-equiboots">
<h3><strong>Step 1: Import and Initialize EquiBoots</strong><a class="headerlink" href="#step-1-import-and-initialize-equiboots" title="Link to this heading"></a></h3>
<p>To begin, we instantiate the <code class="docutils literal notranslate"><span class="pre">EquiBoots</span></code> class with the required inputs: the
true outcome labels (<code class="docutils literal notranslate"><span class="pre">y_test</span></code>), predicted class labels (<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>),
predicted probabilities (<code class="docutils literal notranslate"><span class="pre">y_prob</span></code>), and a DataFrame that holds sensitive
attributes like <code class="docutils literal notranslate"><span class="pre">race</span></code> or <code class="docutils literal notranslate"><span class="pre">sex</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">y_pred</span></code>, <code class="docutils literal notranslate"><span class="pre">y_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">y_test</span></code> are defined inside the <a class="reference internal" href="model_to_pred.html#modeling-generation"><span class="std std-ref">modeling generation section</span></a>.</p>
</div>
<p>Once initialized, <code class="docutils literal notranslate"><span class="pre">EquiBoots</span></code> uses its internal grouping mechanism to enable
fairness auditing by slicing the dataset into mutually exclusive subgroups based
on each fairness variable. This slicing is a prerequisite for evaluating model
behavior across subpopulations.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">grouper</span></code> method stores index-level membership for each group, ensuring
that only groups meeting a minimum sample size are considered. This prevents
unstable or misleading metric calculations. Once sliced, we call <code class="docutils literal notranslate"><span class="pre">slicer</span></code>
to extract the <code class="docutils literal notranslate"><span class="pre">y_true</span></code>, <code class="docutils literal notranslate"><span class="pre">y_pred</span></code>, and <code class="docutils literal notranslate"><span class="pre">y_prob</span></code> values corresponding to
each group. Finally, <code class="docutils literal notranslate"><span class="pre">get_metrics</span></code> is used to compute core performance metrics
for each subgroup.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">equiboots</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">eqb</span>

<span class="c1"># Create fairness DataFrame</span>
<span class="n">fairness_df</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[[</span><span class="s1">&#39;race&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>

<span class="n">eq</span> <span class="o">=</span> <span class="n">eqb</span><span class="o">.</span><span class="n">EquiBoots</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
    <span class="n">y_prob</span><span class="o">=</span><span class="n">y_prob</span><span class="p">,</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span>
    <span class="n">fairness_df</span><span class="o">=</span><span class="n">fairness_df</span><span class="p">,</span>
    <span class="n">fairness_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-2-slice-groups-and-compute-point-estimates">
<h3><strong>Step 2: Slice Groups and Compute Point Estimates</strong><a class="headerlink" href="#step-2-slice-groups-and-compute-point-estimates" title="Link to this heading"></a></h3>
<p>Once the class is initialized, we slice the dataset into subgroups and compute
performance metrics for each group. This step is critical for assessing whether
model performance varies by group.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">equiboots</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">eqb</span>

<span class="n">sliced_race_data</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">slicer</span><span class="p">(</span><span class="s2">&quot;race&quot;</span><span class="p">)</span>
<span class="n">race_metrics</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(</span><span class="n">sliced_race_data</span><span class="p">)</span>

<span class="n">sliced_sex_data</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">slicer</span><span class="p">(</span><span class="s2">&quot;sex&quot;</span><span class="p">)</span>
<span class="n">sex_metrics</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(</span><span class="n">sliced_sex_data</span><span class="p">)</span>
</pre></div>
</div>
<p>Each output is a dictionary of group names (e.g., <code class="docutils literal notranslate"><span class="pre">'Male'</span></code>, <code class="docutils literal notranslate"><span class="pre">'Female'</span></code>, <code class="docutils literal notranslate"><span class="pre">'Asian'</span></code>, <code class="docutils literal notranslate"><span class="pre">'White'</span></code>)
mapped to performance metrics such as accuracy, AUROC, precision, or RMSE, depending on the task type.</p>
</section>
</section>
<section id="metrics-dataframe">
<h2>Metrics DataFrame<a class="headerlink" href="#metrics-dataframe" title="Link to this heading"></a></h2>
<p>Because these dictionaries can contain many entries and nested metric structures,
we avoid printing them directly in documentation. Instead, we use the <code class="docutils literal notranslate"><span class="pre">metrics_dataframe()</span></code>
function to transform the dictionary into a clean, filterable DataFrame.</p>
<p>To keep the table concise and relevant, we subset the DataFrame to include only a selected set of metrics:</p>
<ul class="simple">
<li><p><cite>Accuracy</cite></p></li>
<li><p><cite>Precision</cite></p></li>
<li><p><cite>Recall</cite></p></li>
<li><p><cite>F1 Score</cite></p></li>
<li><p><cite>Specificity</cite></p></li>
<li><p><cite>TP Rate</cite></p></li>
<li><p><cite>Prevalence</cite></p></li>
<li><p><cite>Average Precision Score</cite></p></li>
<li><p><cite>Calibration AUC</cite></p></li>
</ul>
<dl class="py function">
<dt class="sig sig-object py" id="metrics_dataframe">
<span class="sig-name descname"><span class="pre">metrics_dataframe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metrics_data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_dataframe" title="Link to this definition"></a></dt>
<dd><p>Transforms a list of grouped metric dictionaries into a single flat DataFrame.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>metrics_data</strong> (<em>List</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>]</em><em>]</em><em>]</em>) – A list of dictionaries, where each dictionary maps a group name to its associated performance metrics.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tidy DataFrame with one row per group and one column per metric. The group names are stored in the <code class="docutils literal notranslate"><span class="pre">attribute_value</span></code> column.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pd.DataFrame</p>
</dd>
</dl>
</dd></dl>

<blockquote id="note">
<div><ul class="simple">
<li><p>This function is used after computing metrics using <code class="docutils literal notranslate"><span class="pre">eqb.get_metrics()</span></code>.</p></li>
<li><p>It flattens nested group-wise dictionaries into a readable table, enabling easy subsetting, filtering, and export.</p></li>
<li><p>Common use cases include displaying fairness-related metrics such as Accuracy, Precision, Recall, Specificity, Calibration AUC, and others across different sensitive attribute groups (e.g., race, sex).</p></li>
</ul>
</div></blockquote>
<p>The <code class="docutils literal notranslate"><span class="pre">metrics_dataframe()</span></code> function simplifies post-processing and reporting by converting the raw output of group-level metrics into a tabular format. Each row corresponds to a demographic group, and each column represents a different metric.</p>
<p>Below is an example of how this function is used in practice to format metrics by race:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">equiboots</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">eqb</span>

<span class="n">race_metrics_df</span> <span class="o">=</span> <span class="n">eqb</span><span class="o">.</span><span class="n">metrics_dataframe</span><span class="p">(</span><span class="n">metrics_data</span><span class="o">=</span><span class="n">race_metrics</span><span class="p">)</span>
<span class="n">race_metrics_df</span> <span class="o">=</span> <span class="n">race_metrics_df</span><span class="p">[</span>
    <span class="p">[</span>
        <span class="s2">&quot;attribute_value&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Precision&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Recall&quot;</span><span class="p">,</span>
        <span class="s2">&quot;F1 Score&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Specificity&quot;</span><span class="p">,</span>
        <span class="s2">&quot;TP Rate&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Prevalence&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Average Precision Score&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Calibration AUC&quot;</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">]</span>
<span class="c1">## round to 3 decimal places for readability</span>
<span class="nb">round</span><span class="p">(</span><span class="n">race_metrics_df</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>This yields a structured and readable table of group-level performance for use in reporting or further analysis.</p>
<p><strong>Output</strong></p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-2b7s{text-align:right;vertical-align:bottom}
.tg .tg-8d8j{text-align:center;vertical-align:bottom}
.tg .tg-kex3{font-weight:bold;text-align:right;vertical-align:bottom}
@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}</style>
<div class="tg-wrap"><table class="tg"><thead>
<tr>
    <th class="tg-8d8j"></th>
    <th class="tg-kex3">attribute_value</th>
    <th class="tg-kex3">Accuracy</th>
    <th class="tg-kex3">Precision</th>
    <th class="tg-kex3">Recall</th>
    <th class="tg-kex3">F1 Score</th>
    <th class="tg-kex3">Specificity</th>
    <th class="tg-kex3">TP Rate</th>
    <th class="tg-kex3">Prevalence</th>
    <th class="tg-kex3">Calibration AUC</th>
</tr></thead>
<tbody>
<tr>
    <td class="tg-8d8j">0</td>
    <td class="tg-2b7s">White</td>
    <td class="tg-2b7s">0.853</td>
    <td class="tg-2b7s">0.761</td>
    <td class="tg-2b7s">0.638</td>
    <td class="tg-2b7s">0.694</td>
    <td class="tg-2b7s">0.929</td>
    <td class="tg-2b7s">0.638</td>
    <td class="tg-2b7s">0.262</td>
    <td class="tg-2b7s">0.040</td>
</tr>
<tr>
    <td class="tg-8d8j">1</td>
    <td class="tg-2b7s">Black</td>
    <td class="tg-2b7s">0.931</td>
    <td class="tg-2b7s">0.861</td>
    <td class="tg-2b7s">0.549</td>
    <td class="tg-2b7s">0.670</td>
    <td class="tg-2b7s">0.987</td>
    <td class="tg-2b7s">0.549</td>
    <td class="tg-2b7s">0.128</td>
    <td class="tg-2b7s">0.054</td>
</tr>
<tr>
    <td class="tg-8d8j">2</td>
    <td class="tg-2b7s">Asian-Pac-Islander</td>
    <td class="tg-2b7s">0.826</td>
    <td class="tg-2b7s">0.760</td>
    <td class="tg-2b7s">0.543</td>
    <td class="tg-2b7s">0.633</td>
    <td class="tg-2b7s">0.934</td>
    <td class="tg-2b7s">0.543</td>
    <td class="tg-2b7s">0.277</td>
    <td class="tg-2b7s">0.140</td>
</tr>
<tr>
    <td class="tg-8d8j">3</td>
    <td class="tg-2b7s">Amer-Indian-Eskimo</td>
    <td class="tg-2b7s">0.879</td>
    <td class="tg-2b7s">0.444</td>
    <td class="tg-2b7s">0.364</td>
    <td class="tg-2b7s">0.400</td>
    <td class="tg-2b7s">0.943</td>
    <td class="tg-2b7s">0.364</td>
    <td class="tg-2b7s">0.111</td>
    <td class="tg-2b7s">0.323</td>
</tr>
<tr>
    <td class="tg-8d8j">4</td>
    <td class="tg-2b7s">Other</td>
    <td class="tg-2b7s">0.958</td>
    <td class="tg-2b7s">1.000</td>
    <td class="tg-2b7s">0.500</td>
    <td class="tg-2b7s">0.667</td>
    <td class="tg-2b7s">1.000</td>
    <td class="tg-2b7s">0.500</td>
    <td class="tg-2b7s">0.083</td>
    <td class="tg-2b7s">0.277</td>
</tr>
</tbody></table></div><div style="height: 40px;"></div></section>
<section id="statistical-tests">
<h2>Statistical Tests<a class="headerlink" href="#statistical-tests" title="Link to this heading"></a></h2>
<p>After computing point estimates for different demographic groups, we may want to
assess whether observed differences in model performance are statistically significant.
This is particularly important when determining if disparities are due to random
variation or reflect systematic bias.</p>
<p>EquiBoots provides a method to conduct hypothesis testing across group-level metrics.
The <code class="docutils literal notranslate"><span class="pre">analyze_statistical_significance</span></code> function performs appropriate statistical
tests—such as Chi-square tests for classification tasks—while supporting multiple
comparison adjustments.</p>
<dl class="py function">
<dt class="sig sig-object py" id="analyze_statistical_significance">
<span class="sig-name descname"><span class="pre">analyze_statistical_significance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">differences</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#analyze_statistical_significance" title="Link to this definition"></a></dt>
<dd><p><strong>Performs statistical significance testing of metric differences between groups.</strong></p>
<p>This method compares model performance across subgroups (e.g., race, sex) to determine whether the differences in metrics (e.g., accuracy, F1 score) are statistically significant. It supports multiple test types and adjustment methods for robust group-level comparison.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Dictionary of metrics returned by <code class="docutils literal notranslate"><span class="pre">get_metrics()</span></code>, where each key is a group name and values are metric dictionaries.</p></li>
<li><p><strong>var_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The name of the sensitive attribute or grouping variable (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;race&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;sex&quot;</span></code>).</p></li>
<li><p><strong>test_config</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – <p>Configuration dictionary defining how the statistical test is performed. The following keys are supported:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">test_type</span></code>: Type of test to use (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;chi_square&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;bootstrap&quot;</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code>: Significance threshold (default: 0.05).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">adjust_method</span></code>: Correction method for multiple comparisons (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;bonferroni&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;fdr_bh&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;holm&quot;</span></code>, or <code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">confidence_level</span></code>: Confidence level used to compute intervals (e.g., <code class="docutils literal notranslate"><span class="pre">0.95</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">classification_task</span></code>: Specify if the model task is <code class="docutils literal notranslate"><span class="pre">&quot;binary_classification&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;multiclass_classification&quot;</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>differences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>, </em><em>optional</em>) – Optional precomputed list of raw metric differences (default is <code class="docutils literal notranslate"><span class="pre">None</span></code>; typically not required).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A nested dictionary containing statistical test results for each metric, with each value being a <code class="docutils literal notranslate"><span class="pre">StatTestResult</span></code> object that includes:</p>
<ul class="simple">
<li><p>test statistic</p></li>
<li><p>raw and adjusted p-values</p></li>
<li><p>confidence intervals</p></li>
<li><p>significance flags (<code class="docutils literal notranslate"><span class="pre">True</span></code> / <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p>effect sizes (e.g., Cohen’s d, rank-biserial correlation)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a>, StatTestResult]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">test_config</span></code> is not provided or is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
</dl>
</dd></dl>

<p>This function returns a dictionary where each key is a metric name and the
corresponding value is another dictionary mapping each group to its <code class="docutils literal notranslate"><span class="pre">StatTestResult</span></code>.</p>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading"></a></h3>
<p>The following example demonstrates how to configure and run these tests on
performance metrics for the <code class="docutils literal notranslate"><span class="pre">race</span></code> and <code class="docutils literal notranslate"><span class="pre">sex</span></code> subgroups:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;test_type&quot;</span><span class="p">:</span> <span class="s2">&quot;chi_square&quot;</span><span class="p">,</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="s2">&quot;adjust_method&quot;</span><span class="p">:</span> <span class="s2">&quot;bonferroni&quot;</span><span class="p">,</span>
    <span class="s2">&quot;confidence_level&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="s2">&quot;classification_task&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_classification&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">stat_test_results_race</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">analyze_statistical_significance</span><span class="p">(</span>
    <span class="n">race_metrics</span><span class="p">,</span> <span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="n">test_config</span>
<span class="p">)</span>

<span class="n">stat_test_results_sex</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">analyze_statistical_significance</span><span class="p">(</span>
    <span class="n">sex_metrics</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">,</span> <span class="n">test_config</span>
<span class="p">)</span>

<span class="n">overall_stat_results</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;sex&quot;</span><span class="p">:</span> <span class="n">stat_test_results_sex</span><span class="p">,</span>
    <span class="s2">&quot;race&quot;</span><span class="p">:</span> <span class="n">stat_test_results_race</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="statistical-significance-plots">
<h2>Statistical Significance Plots<a class="headerlink" href="#statistical-significance-plots" title="Link to this heading"></a></h2>
<p>EquiBoots supports formal statistical testing to assess whether differences in
performance metrics across demographic groups are statistically significant.</p>
<p>When auditing models for fairness, it’s important not just to observe differences
in metrics like accuracy or recall, but to determine whether these differences are
<strong>statistically significant</strong>. EquiBoots provides built-in support for this analysis
via omnibus and pairwise statistical tests.</p>
<section id="test-setup">
<h3>Test Setup<a class="headerlink" href="#test-setup" title="Link to this heading"></a></h3>
<ul>
<li><p>EquiBoots uses <strong>chi-square tests</strong> to evaluate:</p>
<ul class="simple">
<li><p>Whether overall performance disparities across groups are significant (omnibus test).</p></li>
<li><p>If so, which specific groups significantly differ from the reference (pairwise tests).</p></li>
</ul>
</li>
<li><p>Reference groups for each fairness variable can be set manually during class initialization using the <code class="docutils literal notranslate"><span class="pre">reference_groups</span></code> parameter:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">eq</span> <span class="o">=</span> <span class="n">EquiBoots</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=...</span><span class="p">,</span>
    <span class="n">y_pred</span><span class="o">=...</span><span class="p">,</span>
    <span class="n">y_prob</span><span class="o">=...</span><span class="p">,</span>
    <span class="n">fairness_df</span><span class="o">=...</span><span class="p">,</span>
    <span class="n">fairness_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">],</span>
    <span class="n">reference_groups</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;female&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>Once tests are computed, the <code class="docutils literal notranslate"><span class="pre">eq_group_metrics_point_plot</span></code> function can
visualize point estimates along with statistical significance annotations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">eqb</span><span class="o">.</span><span class="n">eq_group_metrics_point_plot</span><span class="p">(</span>
    <span class="n">group_metrics</span><span class="o">=</span><span class="p">[</span><span class="n">race_metrics</span><span class="p">,</span> <span class="n">sex_metrics</span><span class="p">],</span>
    <span class="n">metric_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;Precision&quot;</span><span class="p">,</span> <span class="s2">&quot;Recall&quot;</span><span class="p">],</span>
    <span class="n">category_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">],</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">include_legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">plot_thresholds</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
    <span class="n">raw_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">show_grid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">y_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">statistical_tests</span><span class="o">=</span><span class="n">overall_stat_results</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<div class="no-click"><a class="reference internal image-reference" href="_images/stat_point_estimate_plot.svg"><img alt="Statistically-Based Point Estimate Plot" class="align-center" src="_images/stat_point_estimate_plot.svg" style="width: 600px;" />
</a>
<div style="height: 40px;"></div><p>The chart above summarizes how model performance varies across race and sex groups
for three key metrics: <strong>Accuracy</strong>, <strong>Precision</strong>, and <strong>Recall</strong>.</p>
<p>Each <strong>subplot</strong> corresponds to a single metric, plotted separately for race (left) and sex (right).</p>
<p>Here’s how to read the plot:</p>
<ul class="simple">
<li><p>Each <strong>point</strong> shows the average metric score for a demographic group.</p></li>
<li><p><strong>Letters (A–G)</strong> label the groups (e.g., A = Amer-Indian-Eskimo, B = Asian-Pac-Islander), with the full mapping provided in the legend.</p></li>
<li><p>The <strong>star symbol (★)</strong> below a group axis label indicates that the <strong>omnibus test</strong> for the corresponding fairness attribute (e.g., race or sex) was statistically significant overall.</p></li>
<li><p>The <strong>triangle symbol (▲)</strong> denotes groups that differ <strong>significantly from the reference group</strong>, as determined by pairwise statistical tests (e.g., Bonferroni-adjusted chi-square).</p></li>
<li><p>Color-coding helps distinguish categories and corresponds to the legend at the top.</p></li>
</ul>
<p>This visualization reveals whether disparities exist not only <strong>numerically</strong>, but also <strong>statistically</strong>, helping validate whether observed group-level differences are likely due to bias or simply random variation.</p>
</section>
</section>
<section id="statistical-metrics-table">
<h2>Statistical Metrics table<a class="headerlink" href="#statistical-metrics-table" title="Link to this heading"></a></h2>
<p>Once statistical tests have been performed, we can summarize the results in a structured table that shows:</p>
<ul class="simple">
<li><p>The <strong>performance metrics</strong> for each group.</p></li>
<li><p>Whether the <strong>omnibus test</strong> detected any significant overall differences.</p></li>
<li><p>Which <strong>individual groups</strong> differ significantly from the reference group.</p></li>
</ul>
<p>This is done using the <code class="docutils literal notranslate"><span class="pre">metrics_table</span></code> function from EquiBoots, which takes in group metrics, test results, and the name of the reference group:</p>
<dl class="py function">
<dt class="sig sig-object py" id="metrics_table">
<span class="sig-name descname"><span class="pre">metrics_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metrics</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">statistical_tests</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">differences</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reference_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#metrics_table" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metrics</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – A dictionary or list of dictionaries containing metric results per group. This can either be point estimate output from <code class="docutils literal notranslate"><span class="pre">get_metrics</span></code> or bootstrapped results.</p></li>
<li><p><strong>statistical_tests</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>optional</em>) – Output from <code class="docutils literal notranslate"><span class="pre">analyze_statistical_significance</span></code> containing omnibus and pairwise test results. If provided, annotations will be added to the output table to reflect significance.</p></li>
<li><p><strong>differences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>optional</em>) – A list of bootstrapped difference dictionaries returned from <code class="docutils literal notranslate"><span class="pre">calculate_differences</span></code>. If provided, the function will average these differences and annotate the results if significant.</p></li>
<li><p><strong>reference_group</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – Name of the reference group used in pairwise comparisons. Only needed if displaying pairwise significance for bootstrapped differences.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A pandas DataFrame where rows are metric names and columns are group names. If <code class="docutils literal notranslate"><span class="pre">statistical_tests</span></code> is provided:
- Omnibus test significance is marked with an asterisk (<code class="docutils literal notranslate"><span class="pre">*</span></code>) next to column names.
- Pairwise group significance (vs. reference) is marked with a triangle (<code class="docutils literal notranslate"><span class="pre">▲</span></code>).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pd.DataFrame</p>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The function supports <strong>both point estimates and bootstrapped results</strong>.</p></li>
<li><p>When using bootstrapped differences, it computes the <strong>mean difference</strong> for each metric across iterations.</p></li>
<li><p>Automatically drops less commonly visualized metrics like Brier Score, Log Loss, and Prevalence for clarity if significance annotations are active.</p></li>
</ul>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stat_metrics_table_point</span> <span class="o">=</span> <span class="n">metrics_table</span><span class="p">(</span>
    <span class="n">race_metrics</span><span class="p">,</span>
    <span class="n">statistical_tests</span><span class="o">=</span><span class="n">stat_test_results_race</span><span class="p">,</span>
    <span class="n">reference_group</span><span class="o">=</span><span class="s2">&quot;White&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can then display the table as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Table with metrics per group and statistical significance shown on</span>
<span class="c1">## columns for omnibus and/or pairwise</span>

<span class="n">stat_metrics_table_point</span>
</pre></div>
</div>
<p>The resulting table displays one row per group and one column per metric. Symbols like <code class="docutils literal notranslate"><span class="pre">*</span></code> and <code class="docutils literal notranslate"><span class="pre">▲</span></code> appear in the appropriate cells to indicate significance:</p>
<ul class="simple">
<li><p>★ marks metrics where the <strong>omnibus test</strong> found significant variation across all groups.</p></li>
<li><p>▲ marks metrics where a specific group differs significantly from the <strong>reference group</strong>.</p></li>
</ul>
<p>This format provides a concise, interpretable snapshot of where disparities are statistically supported in your model outputs.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-2b7s{text-align:right;vertical-align:bottom}
.tg .tg-7zrl{text-align:left;vertical-align:bottom}
.tg .tg-kex3{font-weight:bold;text-align:right;vertical-align:bottom}
@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}</style>
<div class="tg-wrap"><table class="tg"><thead>
<tr>
    <th class="tg-7zrl"></th>
    <th class="tg-kex3">White *</th>
    <th class="tg-kex3">Black * ▲</th>
    <th class="tg-kex3">Asian-Pac-Islander *</th>
    <th class="tg-kex3">Amer-Indian-Eskimo * ▲</th>
    <th class="tg-kex3">Other * ▲</th>
</tr></thead>
<tbody>
<tr>
    <td class="tg-7zrl">Accuracy</td>
    <td class="tg-2b7s">0.853</td>
    <td class="tg-2b7s">0.931</td>
    <td class="tg-2b7s">0.826</td>
    <td class="tg-2b7s">0.879</td>
    <td class="tg-2b7s">0.958</td>
</tr>
<tr>
    <td class="tg-7zrl">Precision</td>
    <td class="tg-2b7s">0.761</td>
    <td class="tg-2b7s">0.861</td>
    <td class="tg-2b7s">0.76</td>
    <td class="tg-2b7s">0.444</td>
    <td class="tg-2b7s">1</td>
</tr>
<tr>
    <td class="tg-7zrl">Recall</td>
    <td class="tg-2b7s">0.638</td>
    <td class="tg-2b7s">0.549</td>
    <td class="tg-2b7s">0.543</td>
    <td class="tg-2b7s">0.364</td>
    <td class="tg-2b7s">0.5</td>
</tr>
<tr>
    <td class="tg-7zrl">F1 Score</td>
    <td class="tg-2b7s">0.694</td>
    <td class="tg-2b7s">0.67</td>
    <td class="tg-2b7s">0.633</td>
    <td class="tg-2b7s">0.4</td>
    <td class="tg-2b7s">0.667</td>
</tr>
<tr>
    <td class="tg-7zrl">Specificity</td>
    <td class="tg-2b7s">0.929</td>
    <td class="tg-2b7s">0.987</td>
    <td class="tg-2b7s">0.934</td>
    <td class="tg-2b7s">0.943</td>
    <td class="tg-2b7s">1</td>
</tr>
<tr>
    <td class="tg-7zrl">TP Rate</td>
    <td class="tg-2b7s">0.638</td>
    <td class="tg-2b7s">0.549</td>
    <td class="tg-2b7s">0.543</td>
    <td class="tg-2b7s">0.364</td>
    <td class="tg-2b7s">0.5</td>
</tr>
<tr>
    <td class="tg-7zrl">FP Rate</td>
    <td class="tg-2b7s">0.071</td>
    <td class="tg-2b7s">0.013</td>
    <td class="tg-2b7s">0.066</td>
    <td class="tg-2b7s">0.057</td>
    <td class="tg-2b7s">0</td>
</tr>
<tr>
    <td class="tg-7zrl">FN Rate</td>
    <td class="tg-2b7s">0.362</td>
    <td class="tg-2b7s">0.451</td>
    <td class="tg-2b7s">0.457</td>
    <td class="tg-2b7s">0.636</td>
    <td class="tg-2b7s">0.5</td>
</tr>
<tr>
    <td class="tg-7zrl">TN Rate</td>
    <td class="tg-2b7s">0.929</td>
    <td class="tg-2b7s">0.987</td>
    <td class="tg-2b7s">0.934</td>
    <td class="tg-2b7s">0.943</td>
    <td class="tg-2b7s">1</td>
</tr>
<tr>
    <td class="tg-7zrl">TP</td>
    <td class="tg-2b7s">1375</td>
    <td class="tg-2b7s">62</td>
    <td class="tg-2b7s">38</td>
    <td class="tg-2b7s">4</td>
    <td class="tg-2b7s">3</td>
</tr>
<tr>
    <td class="tg-7zrl">FP</td>
    <td class="tg-2b7s">432</td>
    <td class="tg-2b7s">10</td>
    <td class="tg-2b7s">12</td>
    <td class="tg-2b7s">5</td>
    <td class="tg-2b7s">0</td>
</tr>
<tr>
    <td class="tg-7zrl">FN</td>
    <td class="tg-2b7s">780</td>
    <td class="tg-2b7s">51</td>
    <td class="tg-2b7s">32</td>
    <td class="tg-2b7s">7</td>
    <td class="tg-2b7s">3</td>
</tr>
<tr>
    <td class="tg-7zrl">TN</td>
    <td class="tg-2b7s">5631</td>
    <td class="tg-2b7s">760</td>
    <td class="tg-2b7s">171</td>
    <td class="tg-2b7s">83</td>
    <td class="tg-2b7s">66</td>
</tr>
<tr>
    <td class="tg-7zrl">Predicted Prevalence</td>
    <td class="tg-2b7s">0.22</td>
    <td class="tg-2b7s">0.082</td>
    <td class="tg-2b7s">0.198</td>
    <td class="tg-2b7s">0.091</td>
    <td class="tg-2b7s">0.042</td>
</tr>
</tbody></table></div><div style="height: 40px;"></div><aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Kohavi, R. (1996). <em>Census Income</em>. UCI Machine Learning Repository. <a class="reference external" href="https://doi.org/10.24432/C5GP7S">https://doi.org/10.24432/C5GP7S</a>.</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Welcome to the EquiBoots Documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bootstrapped_metrics.html" class="btn btn-neutral float-right" title="Bootstrapped Metrics Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright UCLA CTSI ML Team: Leonid Shpaner, Arthur Funnell, Panayiotis Petousis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>